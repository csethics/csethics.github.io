<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Week 2: Race After Technology | Everyday Ethics and Quotidian Quandaries for Computer Scientists</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="https://csethics.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="https://csethics.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://csethics.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://csethics.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="https://csethics.github.io/css/fonts.css">
    <link rel="stylesheet" href="https://csethics.github.io/css/finite.css">
    <link rel="shortcut icon" href="/images/uva.png">  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="https://csethics.github.io/">Everyday Ethics and Quotidian Quandaries for Computer Scientists</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      


	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="https://csethics.github.io/">Everyday Ethics and Quotidian Quandaries for Computer Scientists</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		


	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      

<div class="content">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      <h1 itemprop="name" class="pagetitle">Week 2: Race After Technology</h1>
      <div class="post-body" itemprop="articleBody">
        <h2 id="readings">Readings</h2>
<ul>
<li>Ruha Benjamin, <a href="https://www.ruhabenjamin.com/race-after-technology"><em>Race After Technology</em></a>, Introduction and Chapter 1</li>
<li>John McWhorter, <a href="https://time.com/4475627/is-technology-capable-of-being-racist/"><em>&lsquo;Racist&rsquo; Technology Is a Bug—Not a Crime</em></a>, Time Magazine, 12 September 2016</li>
</ul>
<p>Optional additional readings:</p>
<ul>
<li>Aylin Caliskan, Joanna J. Bryson, Arvind Narayanan. <a href="/docs/caliskan-language-biases.pdf"><em>Semantics derived automatically from language corpora contain human-like biases</em></a>, Science, 14 April 2017.</li>
<li>Latanya Sweeney. <a href="/docs/sweeney-discrimination-ads.pdf"><em>Discrimination in Online Ad Delivery</em></a>, ACM Queue, 2013.</li>
</ul>
<p><strong><a href="https://github.com/csethics/csethics.github.io/discussions/8">On-Line Discussion</a></strong></p>
<h1 id="class-meetings">Class Meetings</h1>
<p><strong>Lead by Team 1</strong></p>
<p><a href="https://www.dropbox.com/s/cmobu905e0yer28/week2-updated.pdf?dl=0">Slides for Week 2 [PDF]</a></p>
<h1 id="blog-summary">Blog Summary</h1>
<p><strong>Written by Team 4</strong></p>
<p>Tuesday&rsquo;s class began with a discussion about names, inspired by Ruha
Benjamin&rsquo;s own &ldquo;What&rsquo;s in a Name?&rdquo; game which she plays with her
undergraduate students at Princeton. In their table groups, students
discussed the backgrounds and assumptions in a name — chosen or
imposed — in sharing the story behind their own name.</p>
<p>Our discussions found that some students with immigrant parents were
given common American-English names (often inspired by popular
musicians and actors), instead of names in other cultures and
languages like their parents. Several students reported that this was
intentional on their parents' parts in order to help their children
better fit into American society and make it easier for other people
to say their name. This exemplified the point Benjamin made in Chapter
1 of her book in describing how common white-sounding names are
considered the standard in the U.S.</p>
<p>Next, the class turned to the debate around free will and the
influence of algorithms on our decisions and behaviors. Students
centered discussion around Benjamin’s quote “The road to inequity is
paved with technical fixes.”</p>
<p>We discussed how Amazon, Facebook, Netflix, Spotify, YouTube, etc. all
recommend specific content (products, posts, tv shows, movies, music)
to users based on the data collected from them— only showing specific
content to people on these large platforms. As a result, it is made
harder to have “free will” to seek out specific content that is not
recommended for you by algorithms. This can lead to “echo chambers”
where users live in their own reverberating bubble of information, and
can lead people down radical political paths.</p>
<p>Another aspect of this discussion was acknowledging that small
“technical fixes” often only mask problems such as racism and gender
inequality by hiding it in the technology we use while not necessarily
addressing these issues in society. For example, sentencing algorithms
used to determine length of jail sentences (which we&rsquo;ll talk more
about in <a href="/week3">Week 3</a>) and other legal criminal decisions in
court.</p>
<p>These sentencing algorithms often give the appearance of placing
sentences into unbiased hands because they take the decision away from
a judge, who can be seen as fallible or biased, and hands it to an
algorithm that is meant to not take into account race or other
identifiers that could lead to discrimination. The issue is, however,
that these algorithms then use factors associated heavily with race,
such as past criminal history, socioeconomic status, crime rates in a
person’s home area, to determine sentencing. The results are often
just as discriminatory as we’d expect from human decisions, in which
POC are sentenced to far longer prison stays than white people who
have committed comparable crimes.</p>
<p>Next, the class tackled the larger question of what technology is
truly racist and how we can determine between racism in tech and
advancements that have merely gone awry. Students began with an
example of the Twitter algorithm which crops photos attached to
tweets. The algorithm used to search for high contrast colors in
pictures to decide what portion of a photo to preview. This caused
pictures containing people of different races to be constantly cropped
to show the white person instead of a POC who is also in the
photo. Once Twitter became aware of this trend in its algorithm, it changed
it’s system to be a standard crop for all photos.</p>
<p>Another example of a case where it’s hard to determine responsibility
and intent behind racist outcomes in tech was Microsoft
TayTweets. TayTweets was the first bot of its kind launched on Twitter
as an experiment to see if AI could learn to interact on social media
like a real person. It’s design, however, left room for some
mistakes. Within a day or two of it’s launch, TayTweets was targeted
by trolls who taught it racist and bigoted phrases, and TayTweets
began repeating and using those phrases. (See James Mickens' <a href="https://youtu.be/ajGX7odA87k?t=1184">keynote
talk at USENIX Security 2018</a> for
a entertaining summary of this unfortunate episode.)</p>
<p>During this discussion, some made an argument that since Microsoft had
good intentions in launching this new technology, and didn’t predict
the racist outcome, it’s really the companies that have followed and
created similar bots after seeing TayTweets’ poor outcome who should
be held accountable as they aren’t learning from racist mistakes and
taking actions to avoid them.</p>
<h2 id="thursdays-class">Thursday&rsquo;s Class</h2>
<p>Before the second class period, students were tasked with completing
an Implicit Association Test and finding a search query that displayed
bias in the results. Some groups argued that the measurement of speed
in the test results judged reaction time more than actual implicit
bias. Most of the group realized that they had a slight bias for one
or the other ethnicity, and discussed methods for taking into account
their own implicit bias, especially within technology. Some groups
came to the consensus that having ethnically diverse teams limits
implicit bias namely in the production of technology.</p>
<p>In group discussions on the second homework question, some students
discussed how it was very hard to find a specific query that displayed
bias. Several students thought that when searching “school children”,
the image results forced specific ethnic representation in the
results. In other words, it seemed that the search engine was
protecting its image by adding representation. Furthermore, students
wondered if changing geographic location to a low socioeconomic area
produces different results than a more affluent area.</p>
<p>The class then moved into discussing the optional readings. Key
mentions here were word embedding and its reflection of bias,
discrimination in ad delivery, good training data, quantifying bias
through sentiment analysis, and whether or not the developer has
responsibility to over correct social bias as opposed to simply being
aware and not reinforcing it. Some students reached a consensus that
it is not the role of software developers to over-correct for societal
biases, but rather to be aware and stop using a technology when it
does perpetuate it.</p>

      </div>

      <meta itemprop="wordCount" content="1057">
      <meta itemprop="datePublished" content="2021-02-04">
      <meta itemprop="url" content="https://csethics.github.io/week2/">
    </article>
</div>


    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-12 medium-6">
      <a href="//csethics.github.io"><b>cs 3501: Everyday Ethics for Computer Scientists</b></a><br>
      <a href="//www.cs.virginia.edu">University of Virginia</a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="https://csethics.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
  </hr>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
